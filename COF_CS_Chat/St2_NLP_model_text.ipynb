{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading current QuestionSet to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from random import sample\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout,  Conv1D, MaxPooling2D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the Chat Corpus\n",
    "chat = pd.read_csv('./data/ChatCorpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the Answer Corpus\n",
    "ans_file = pd.read_csv('./data/IntentAnswers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>question_main</th>\n",
       "      <th>intent</th>\n",
       "      <th>QuestionWords</th>\n",
       "      <th>q_perm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             family  \\\n",
       "0  Applying and account information   \n",
       "1  Applying and account information   \n",
       "\n",
       "                                       question_main     intent  \\\n",
       "0  What information does Banco Uno® require when ...  Cardapply   \n",
       "1  What information does Banco Uno® require when ...  Cardapply   \n",
       "\n",
       "                          QuestionWords                           q_perm  \n",
       "0  what how apply information need card  what how apply information need  \n",
       "1  what how apply information need card  what how apply information card  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_perm</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21137</th>\n",
       "      <td>answer increase line rent question</td>\n",
       "      <td>clrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35754</th>\n",
       "      <td>redeem travel than miles for</td>\n",
       "      <td>rewards9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>credit update card find application</td>\n",
       "      <td>CardAppStatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33545</th>\n",
       "      <td>miss don’t earn rewards pay</td>\n",
       "      <td>rewards2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21655</th>\n",
       "      <td>credit line question increase limit</td>\n",
       "      <td>clspend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    q_perm         intent\n",
       "21137   answer increase line rent question         clrent\n",
       "35754         redeem travel than miles for       rewards9\n",
       "1237   credit update card find application  CardAppStatus\n",
       "33545          miss don’t earn rewards pay       rewards2\n",
       "21655  credit line question increase limit        clspend"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat[['q_perm', 'intent']].sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardapply</td>\n",
       "      <td>You’ll need to provide personal information, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Authuser</td>\n",
       "      <td>Yes. You can add authorized users online after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CardAppStatus</td>\n",
       "      <td>If you apply by phone or online, you will ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewCCReceive</td>\n",
       "      <td>If you’re approved, you’ll receive your Banco ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyPayments</td>\n",
       "      <td>We generally apply payments up to your minimum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          intent                                             answer\n",
       "0      Cardapply  You’ll need to provide personal information, i...\n",
       "1       Authuser  Yes. You can add authorized users online after...\n",
       "2  CardAppStatus  If you apply by phone or online, you will ofte...\n",
       "3   NewCCReceive  If you’re approved, you’ll receive your Banco ...\n",
       "4  ApplyPayments  We generally apply payments up to your minimum..."
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intent</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cardapply</th>\n",
       "      <td>You’ll need to provide personal information, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Authuser</th>\n",
       "      <td>Yes. You can add authorized users online after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CardAppStatus</th>\n",
       "      <td>If you apply by phone or online, you will ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewCCReceive</th>\n",
       "      <td>If you’re approved, you’ll receive your Banco ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplyPayments</th>\n",
       "      <td>We generally apply payments up to your minimum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          answer\n",
       "intent                                                          \n",
       "Cardapply      You’ll need to provide personal information, i...\n",
       "Authuser       Yes. You can add authorized users online after...\n",
       "CardAppStatus  If you apply by phone or online, you will ofte...\n",
       "NewCCReceive   If you’re approved, you’ll receive your Banco ...\n",
       "ApplyPayments  We generally apply payments up to your minimum..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set Intent as key for answers\n",
    "ans = ans_file.set_index('intent')\n",
    "ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You’ll need to provide personal information, including your:Full name, Social Security number, Date of birth, Physical address, (No P.O. Boxes) Estimated gross annual income, Checking and/or savings account information'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.loc['Cardapply'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "chat['processed_q'] =chat['q_perm'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "# Remove all the special characters\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "chat['processed_q'] = chat['processed_q'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "\n",
    "# removing dashes \n",
    "chat['processed_q'] = [row.replace('-', '') for row in chat['q_perm']]\n",
    "#chat['processed_q'] = chat['q_perm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing all rows\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d.]+|S+')\n",
    "chat['processed_q'] = [tokenizer.tokenize(row.lower()) for row in chat['processed_q']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoining list of words in each row\n",
    "chat['processed_q'] = [' '.join(row) for row in chat['processed_q']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>question_main</th>\n",
       "      <th>intent</th>\n",
       "      <th>QuestionWords</th>\n",
       "      <th>q_perm</th>\n",
       "      <th>processed_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information need</td>\n",
       "      <td>what how apply information need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information card</td>\n",
       "      <td>what how apply information card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply need information</td>\n",
       "      <td>what how apply need information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply need card</td>\n",
       "      <td>what how apply need card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply card information</td>\n",
       "      <td>what how apply card information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             family  \\\n",
       "0  Applying and account information   \n",
       "1  Applying and account information   \n",
       "2  Applying and account information   \n",
       "3  Applying and account information   \n",
       "4  Applying and account information   \n",
       "\n",
       "                                       question_main     intent  \\\n",
       "0  What information does Banco Uno® require when ...  Cardapply   \n",
       "1  What information does Banco Uno® require when ...  Cardapply   \n",
       "2  What information does Banco Uno® require when ...  Cardapply   \n",
       "3  What information does Banco Uno® require when ...  Cardapply   \n",
       "4  What information does Banco Uno® require when ...  Cardapply   \n",
       "\n",
       "                          QuestionWords                           q_perm  \\\n",
       "0  what how apply information need card  what how apply information need   \n",
       "1  what how apply information need card  what how apply information card   \n",
       "2  what how apply information need card  what how apply need information   \n",
       "3  what how apply information need card         what how apply need card   \n",
       "4  what how apply information need card  what how apply card information   \n",
       "\n",
       "                       processed_q  \n",
       "0  what how apply information need  \n",
       "1  what how apply information card  \n",
       "2  what how apply need information  \n",
       "3         what how apply need card  \n",
       "4  what how apply card information  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing, lemmatizing and joining the words again to feed into the pipeline for modelling\n",
    "\n",
    "def tok_lem_remstop_join(dataset, var):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset_copy = dataset.copy()\n",
    "    new_text = []\n",
    "    for i in dataset_copy[var]:\n",
    "        tokens = word_tokenize(i.lower())\n",
    "        tokens_lem = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens_filtered= [word for word in tokens_lem if not word in stopwords.words('english')]\n",
    "        #tokens_filtered = tokens_lem\n",
    "        new_review = \" \".join(tokens_filtered)\n",
    "        new_text.append(new_review)\n",
    "    dataset_copy[var] = new_text\n",
    "    return dataset_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column without stopwords\n",
    "chat = tok_lem_remstop_join(chat, 'processed_q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>question_main</th>\n",
       "      <th>intent</th>\n",
       "      <th>QuestionWords</th>\n",
       "      <th>q_perm</th>\n",
       "      <th>processed_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information need</td>\n",
       "      <td>apply information need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying and account information</td>\n",
       "      <td>What information does Banco Uno® require when ...</td>\n",
       "      <td>Cardapply</td>\n",
       "      <td>what how apply information need card</td>\n",
       "      <td>what how apply information card</td>\n",
       "      <td>apply information card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             family  \\\n",
       "0  Applying and account information   \n",
       "1  Applying and account information   \n",
       "\n",
       "                                       question_main     intent  \\\n",
       "0  What information does Banco Uno® require when ...  Cardapply   \n",
       "1  What information does Banco Uno® require when ...  Cardapply   \n",
       "\n",
       "                          QuestionWords                           q_perm  \\\n",
       "0  what how apply information need card  what how apply information need   \n",
       "1  what how apply information need card  what how apply information card   \n",
       "\n",
       "              processed_q  \n",
       "0  apply information need  \n",
       "1  apply information card  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating multiple rows so that X/y splits can happen and model can train\n",
    "#chat_dup = pd.concat([chat]*10, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chat['processed_q']\n",
    "y = chat['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36615,)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BELOW ROWS ARE TO GET DIFF PROCESSED QUEUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELLING STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing tags w/default params\n",
    "tvec = TfidfVectorizer()\n",
    "X_train_tvec = tvec.fit_transform(X_train)\n",
    "X_test_tvec = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27461x125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112652 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing w/different tuning to compare models to default\n",
    "tv = TfidfVectorizer(ngram_range=(1,2), max_df=2.0, min_df=5)\n",
    "X_train_tv = tv.fit_transform(X_train)\n",
    "X_test_tv = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression w/tuned vectorizer\n",
    "lr = LogisticRegression(max_iter = 1000, solver= 'liblinear', C=100)\n",
    "lr.fit(X_train_tv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9968682859327774\n",
      "Test score: 0.9954118418177845\n"
     ]
    }
   ],
   "source": [
    "# scoring logistic regression\n",
    "print(f'Train score: {lr.score(X_train_tv, y_train)}')\n",
    "print(f'Test score: {lr.score(X_test_tv, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tv.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model for Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=2.0, min_df=5, ngram_range=(1, 2))),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=100, max_iter=1000, random_state=42,\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing Model for Streamlit\n",
    "# creating pipeline for model\n",
    "nlp_pipe = Pipeline([('tvec', TfidfVectorizer(ngram_range=(1,2), max_df=2.0, min_df=5)), \n",
    "                ('logreg', LogisticRegression(max_iter=1000, random_state=42, solver='liblinear', C=100))])\n",
    "# fitting pipeline\n",
    "nlp_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9968682859327774, 0.9954118418177845)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scoring pipeline to make sure scores still accurate\n",
    "nlp_pipe.score(X_train, y_train), nlp_pipe.score(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cardapply'], dtype=object)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing model on function output and it's performing well\n",
    "input_text='How do i apply for a card?'\n",
    "nlp_pipe.predict([input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greeting'], dtype=object)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing model on function output and it's performing well\n",
    "input_text='hello?'\n",
    "nlp_pipe.predict([input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model to pickle for usage in streamlit\n",
    "pickle.dump(nlp_pipe, open('./models/cs_model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOING SOME PREDICTIONS with raw model\n",
    "# function to preprocess user inputs for nlp model\n",
    "def preprocess_nlp(question):\n",
    "    input_list = []\n",
    "    processed_question = question.replace('-', '')\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d.]+|S+')\n",
    "    token = tokenizer.tokenize(processed_question.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = [lemmatizer.lemmatize(word) for word in token]\n",
    "    joined_text = ' '.join(lem_token)\n",
    "    input_list.append(joined_text)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You’ll need to provide personal information, including your:Full name, Social Security number, Date of birth, Physical address, (No P.O. Boxes) Estimated gross annual income, Checking and/or savings account information'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing inputs for nlp model\n",
    "question_text='how do i apply for a card'\n",
    "input_text = preprocess_nlp(question_text)\n",
    "answer_nlp = nlp_pipe.predict(input_text)\n",
    "ans.loc[answer_nlp[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    hello\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello from Banco Uno.  How can I help you?'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing predictions with pipe\n",
    "question_text='hello'\n",
    "input_text = preprocess_nlp(question_text)\n",
    "ip_series = pd.Series(input_text)\n",
    "print(ip_series)\n",
    "answer_nlp = nlp_pipe.predict(input_text)\n",
    "ans.loc[answer_nlp[0]][0]\n",
    "#ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tv = np.array(X_train_tv)\n",
    "X_test_tv = np.array(X_test_tv)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "#X_train = list(X_train_arr)\n",
    "#y_train = list(y_train)\n",
    "#print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<27461x875 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 198187 stored elements in Compressed Sparse Row format>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Perform Convolution\n",
    "model.add(Conv1D(64, 2, activation = 'relu', input_shape=(27461,1)))\n",
    "#Perform maxpooling\n",
    "model.add(MaxPooling1D())\n",
    "\n",
    "#Perform 2nd COnv and pool\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "\n",
    "#Conv connected to dense layer that is fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(1, activation='softmax'))#sigmoid since binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-13758e8e09b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#fitting and saving the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m hist = model.fit((X_train_tv), (y_train), \n\u001b[0m\u001b[0;32m      3\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  epochs=20, batch_size=20, verbose=1)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m                **kwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m-> 1404\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1405\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1410\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1411\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix)."
     ]
    }
   ],
   "source": [
    "#fitting and saving the model \n",
    "hist = model.fit((X_train_tv), (y_train), \n",
    "                 validation_data=(X_test_tv, y_test),\n",
    "                 epochs=20, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix, numpy.ndarray)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tv), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-8abb414f6cba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m               \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m---> 20\u001b[1;33m history = model.fit(X_train_tv, y_train,\n\u001b[0m\u001b[0;32m     21\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m                **kwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m-> 1404\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1405\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1410\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1411\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix)."
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "#Source: https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c#:~:text=Just%20like%20sentence%20classification%20%2C%20CNN,Textual%20Summarization%2C%20Answer%20Selection%20etc.&text=Just%20like%20sentence%20classification%20%2C%20CNN,Textual%20Summarization%2C%20Answer%20Selection%20etc.\n",
    "#X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "#vocab_size = len(tokenizer.word_index) + 1                          \n",
    "vocab_size=10000\n",
    "maxlen=100\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_tv, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(m, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the best logistic regression score with all variations of hypertuning including vectorizer. The score only improved by 1 point over the null model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOING SOME PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess user inputs for nlp model\n",
    "def preprocess_nlp(question):\n",
    "    input_list = []\n",
    "    processed_question = question.replace('-', '')\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d.]+|S+')\n",
    "    token = tokenizer.tokenize(processed_question.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = [lemmatizer.lemmatize(word) for word in token]\n",
    "    joined_text = ' '.join(lem_token)\n",
    "    input_list.append(joined_text)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    hello how do i apply for a card\n",
      "dtype: object\n",
      "(1, 902)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You’ll need to provide personal information, including your:Full name, Social Security number, Date of birth, Physical address, (No P.O. Boxes) Estimated gross annual income, Checking and/or savings account information'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing inputs for nlp model\n",
    "question_text='hello, how do i  apply for a card'\n",
    "input_text = preprocess_nlp(question_text)\n",
    "ip_series = pd.Series(input_text)\n",
    "print(ip_series)\n",
    "ip_tvec = tv.transform(ip_series)\n",
    "print(ip_tvec.shape)\n",
    "predicted_status_nlp = lr.predict(ip_tvec)\n",
    "ans.loc[predicted_status_nlp[0]][0]\n",
    "\n",
    "#ans.loc['Cardapply'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.26659590e-04, 7.92055084e-04, 7.41358376e-04, 8.26806020e-04,\n",
       "        1.02440693e-03, 8.84519637e-04, 7.21249391e-04, 3.83593759e-03,\n",
       "        7.65979028e-04, 8.64622506e-04, 8.47472219e-04, 8.04103165e-03,\n",
       "        4.70636028e-01, 7.50448732e-04, 8.30442082e-04, 9.88769361e-04,\n",
       "        4.58830887e-01, 8.37186697e-04, 8.47443880e-04, 1.01132414e-03,\n",
       "        6.36285553e-04, 1.04405453e-03, 8.17536790e-04, 1.99506214e-03,\n",
       "        6.69678702e-04, 9.76427246e-04, 5.31190613e-04, 8.78042406e-04,\n",
       "        8.35751599e-04, 9.49845668e-04, 1.01351768e-03, 9.46120563e-04,\n",
       "        6.17869030e-04, 6.18765385e-04, 9.53381932e-04, 1.02506165e-02,\n",
       "        9.66420841e-04, 8.34150521e-04, 9.78406368e-04, 4.52005921e-04,\n",
       "        9.77885571e-04, 9.26590742e-04, 8.77233680e-04, 9.65940442e-04,\n",
       "        4.41202949e-03, 6.15210429e-04, 8.19794961e-04, 8.90512047e-04,\n",
       "        9.49688952e-04, 8.25334019e-04, 7.05060272e-04, 8.94980239e-04,\n",
       "        9.05044518e-04, 8.32719385e-04, 9.95470468e-04, 5.35364312e-04,\n",
       "        5.80839050e-04, 5.20543372e-04]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(ip_tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multinomialNB model w/default vectorizer\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7627830815336608\n",
      "Test score: 0.7518999073215941\n"
     ]
    }
   ],
   "source": [
    "# scoring for hypertuned vectorizer\n",
    "print(f'Train score: {mnb.score(X_train_tv, y_train)}')\n",
    "print(f'Test score: {mnb.score(X_test_tv, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7486019711434486\n",
      "Test score: 0.7490268767377201\n"
     ]
    }
   ],
   "source": [
    "# scoring for default vectorizer\n",
    "print(f'Train score: {mnb.score(X_train_tvec, y_train)}')\n",
    "print(f'Test score: {mnb.score(X_test_tvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypertuned vectorizer scored better than the default with the MultinomialNB. However, the score is still very close to the null scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "ada.fit(X_train_tvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7664287700435629\n",
      "Test score: 0.758758109360519\n"
     ]
    }
   ],
   "source": [
    "# scoring\n",
    "print(f'Train score: {ada.score(X_train_tvec, y_train)}')\n",
    "print(f'Test score: {ada.score(X_test_tvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.766459665708901\n",
      "Test score: 0.7574606116774791\n"
     ]
    }
   ],
   "source": [
    "# scoring \n",
    "print(f'Train score: {ada.score(X_train_tvec, y_train)}')\n",
    "print(f'Test score: {ada.score(X_test_tvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7660889177248432\n",
      "Test score: 0.7524559777571825\n"
     ]
    }
   ],
   "source": [
    "# scoring \n",
    "print(f'Train score: {ada.score(X_train_tv, y_train)}')\n",
    "print(f'Test score: {ada.score(X_test_tv, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost model w/100 estimators scored higher than the previous models using the default vectorizer. It's still not the improvement over the null model we're looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all text columns for Modeling\n",
    "This was supposed to be with joined tags, but they're not. There's not much of a difference between scores with joined or unjoined tags; however, see next notebook for the scoring of those models which were ultimately used for the final nlp model and the streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>DESCRIPTION_TRANSLATED</th>\n",
       "      <th>LOAN_USE</th>\n",
       "      <th>TAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1799331</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Dinnah is 43 years of age and a proud mother o...</td>\n",
       "      <td>to buy farm inputs such as seeds and fertilize...</td>\n",
       "      <td>#Parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1294719</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Resy is a married woman and has been blessed w...</td>\n",
       "      <td>to purchase chicks and poultry feed.</td>\n",
       "      <td>#Animals, #Woman-Owned Business, volunteer_pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1595847</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>0</td>\n",
       "      <td>Lavenda is happily married and has been blesse...</td>\n",
       "      <td>to add stock of beauty products to her salon</td>\n",
       "      <td>user_favorite, #Parent, #Woman-Owned Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1139606</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Hadija is a Kiva borrower. She describes herse...</td>\n",
       "      <td>to buy more stock of vegetables, flour, sugar,...</td>\n",
       "      <td>#Repeat Borrower, #Woman-Owned Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1813411</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Purity, aged 28, is a lovely mother of two chi...</td>\n",
       "      <td>to purchase high-quality seeds and nutrient-ri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LOAN_ID COUNTRY_NAME  STATUS  \\\n",
       "7   1799331        Kenya       1   \n",
       "16  1294719        Kenya       1   \n",
       "21  1595847        Kenya       0   \n",
       "41  1139606        Kenya       1   \n",
       "57  1813411        Kenya       1   \n",
       "\n",
       "                               DESCRIPTION_TRANSLATED  \\\n",
       "7   Dinnah is 43 years of age and a proud mother o...   \n",
       "16  Resy is a married woman and has been blessed w...   \n",
       "21  Lavenda is happily married and has been blesse...   \n",
       "41  Hadija is a Kiva borrower. She describes herse...   \n",
       "57  Purity, aged 28, is a lovely mother of two chi...   \n",
       "\n",
       "                                             LOAN_USE  \\\n",
       "7   to buy farm inputs such as seeds and fertilize...   \n",
       "16               to purchase chicks and poultry feed.   \n",
       "21       to add stock of beauty products to her salon   \n",
       "41  to buy more stock of vegetables, flour, sugar,...   \n",
       "57  to purchase high-quality seeds and nutrient-ri...   \n",
       "\n",
       "                                                 TAGS  \n",
       "7                                             #Parent  \n",
       "16  #Animals, #Woman-Owned Business, volunteer_pic...  \n",
       "21      user_favorite, #Parent, #Woman-Owned Business  \n",
       "41            #Repeat Borrower, #Woman-Owned Business  \n",
       "57                                                NaN  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiva.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51170 entries, 7 to 419149\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   LOAN_ID                 51170 non-null  int64 \n",
      " 1   COUNTRY_NAME            51170 non-null  object\n",
      " 2   STATUS                  51170 non-null  int64 \n",
      " 3   DESCRIPTION_TRANSLATED  50025 non-null  object\n",
      " 4   LOAN_USE                50025 non-null  object\n",
      " 5   TAGS                    43157 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "kiva.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling nulls with blank spaces so they aren't erased\n",
    "kiva['DESCRIPTION_TRANSLATED'] = kiva['DESCRIPTION_TRANSLATED'].fillna(' ')\n",
    "kiva['LOAN_USE'] = kiva['LOAN_USE'].fillna(' ')\n",
    "kiva['TAGS'] = kiva['TAGS'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column with all text included\n",
    "kiva['all_text'] = (kiva['DESCRIPTION_TRANSLATED']+kiva['LOAN_USE']+kiva['TAGS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>DESCRIPTION_TRANSLATED</th>\n",
       "      <th>LOAN_USE</th>\n",
       "      <th>TAGS</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1799331</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Dinnah is 43 years of age and a proud mother o...</td>\n",
       "      <td>to buy farm inputs such as seeds and fertilize...</td>\n",
       "      <td>#Parent</td>\n",
       "      <td>Dinnah is 43 years of age and a proud mother o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1294719</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td>Resy is a married woman and has been blessed w...</td>\n",
       "      <td>to purchase chicks and poultry feed.</td>\n",
       "      <td>#Animals, #Woman-Owned Business, volunteer_pic...</td>\n",
       "      <td>Resy is a married woman and has been blessed w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LOAN_ID COUNTRY_NAME  STATUS  \\\n",
       "7   1799331        Kenya       1   \n",
       "16  1294719        Kenya       1   \n",
       "\n",
       "                               DESCRIPTION_TRANSLATED  \\\n",
       "7   Dinnah is 43 years of age and a proud mother o...   \n",
       "16  Resy is a married woman and has been blessed w...   \n",
       "\n",
       "                                             LOAN_USE  \\\n",
       "7   to buy farm inputs such as seeds and fertilize...   \n",
       "16               to purchase chicks and poultry feed.   \n",
       "\n",
       "                                                 TAGS  \\\n",
       "7                                             #Parent   \n",
       "16  #Animals, #Woman-Owned Business, volunteer_pic...   \n",
       "\n",
       "                                             all_text  \n",
       "7   Dinnah is 43 years of age and a proud mother o...  \n",
       "16  Resy is a married woman and has been blessed w...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiva.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html breaks\n",
    "kiva['all_text'] = kiva['all_text'].str.replace('<br />', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation from text\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d.]+|S+')\n",
    "kiva['all_text'] = [tokenizer.tokenize(row.lower()) for row in kiva['all_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing s's from ends of words to be able to count singulars and plurals together\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "kiva['all_text'] = [[lemmatizer.lemmatize(token) for token in token_list] for token_list in kiva['all_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing most common english words from text\n",
    "kiva['all_text'] = [[token for token in token_list if token not in stopwords.words('english')] for token_list in kiva['all_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoining list of words in each row\n",
    "kiva['joined_text'] = [' '.join(row) for row in kiva['all_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving csv of preprocessed texts for Kenya\n",
    "kiva.to_csv('/Users/precious/dsi/Group-Project/kiva_kenya.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Combined Text Columns for Kenya "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = kiva['joined_text']\n",
    "y2 = kiva['STATUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.784659\n",
       "0    0.215341\n",
       "Name: STATUS, dtype: float64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, stratify=y2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing joined_text data using 1 and 2 grams\n",
    "tdvec = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_tdvec = tdvec.fit_transform(X_train)\n",
    "X_test_tdvec = tdvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tdvec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-94b47c24e059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tdvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tdvec' is not defined"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_tdvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.83013263152409\n",
      "Test score: 0.818103650433831\n"
     ]
    }
   ],
   "source": [
    "# scoring logistic regression\n",
    "print(f'Train score: {logreg.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {logreg.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.848216379602366\n",
      "Test score: 0.8210740248573438\n"
     ]
    }
   ],
   "source": [
    "# scoring logistic regression (ngram 1,2, max_iter1000)\n",
    "print(f'Train score: {logreg.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {logreg.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 1 and 2 ngrams for the vectorizer and max iterations of 1000 for the logistic regression produced our best scores out of all the models with an improvement of 4 points over the null scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating predictions for logreg\n",
    "logreg_preds = logreg.predict(X_test_tdvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6407997883903236\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, logreg_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logreg_cm = confusion_matrix(y_test, logreg_preds)\n",
    "ConfusionMatrixDisplay(logreg_cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.32      0.44      2755\n",
      "           1       0.84      0.96      0.89     10038\n",
      "\n",
      "    accuracy                           0.82     12793\n",
      "   macro avg       0.76      0.64      0.67     12793\n",
      "weighted avg       0.80      0.82      0.80     12793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, logreg_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our logistic regression model is scoring well overall with 82% accuracy, it's not performing well with the expired text data (0). The funded data (1) has low false negatives and predicts 96% correctly (recall score). However, it's only predicting 32% correctly for the expired loans which gives us a high false positive score. With this model we are predicting most loans to be funded and giving those whose text information for their loans are not good enough for funding false hope that they are likely to be funded when they are not for whatever reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_tdvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.999973942726112\n",
      "Test score: 0.8070038302196514\n"
     ]
    }
   ],
   "source": [
    "# scoring with default random forest and default vectorizer\n",
    "print(f'Train score: {rfc.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {rfc.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7846626885895198\n",
      "Test score: 0.7846478542953178\n"
     ]
    }
   ],
   "source": [
    "# scoring with ngrams range of 1,2 in vectorizer\n",
    "print(f'Train score: {rfc.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {rfc.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model is overfit for the default parameters. The increased ngrams in the vectorizer provide a better fit but the score hasn't improved over the null scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=250)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=250)\n",
    "abc.fit(X_train_tdvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8086874951142612\n",
      "Test score: 0.8070819979676386\n"
     ]
    }
   ],
   "source": [
    "# scoring w/default vectorizer and default adaboost\n",
    "print(f'Train score: {abc.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {abc.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8097297860697814\n",
      "Test score: 0.8113812241069335\n"
     ]
    }
   ],
   "source": [
    "# scoring with increased estimators of 250 from 50\n",
    "print(f'Train score: {abc.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {abc.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score improved over the null model but not as well as the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(min_samples_split=3)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_tdvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8228105375615603\n",
      "Test score: 0.8174783084499335\n"
     ]
    }
   ],
   "source": [
    "# scoring \n",
    "print(f'Train score: {gb.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {gb.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is also an improvement over the null model but not as well as the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing logreg score from above to extract features and complete interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rereading in data\n",
    "kiva_kenya = pd.read_csv('/Users/precious/dsi/Group-Project/kiva_kenya.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51170 entries, 0 to 51169\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   LOAN_ID                 51170 non-null  int64 \n",
      " 1   COUNTRY_NAME            51170 non-null  object\n",
      " 2   STATUS                  51170 non-null  int64 \n",
      " 3   DESCRIPTION_TRANSLATED  51170 non-null  object\n",
      " 4   LOAN_USE                51170 non-null  object\n",
      " 5   TAGS                    51170 non-null  object\n",
      " 6   all_text                51170 non-null  object\n",
      " 7   joined_text             51019 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "kiva_kenya.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed null rows not taken into consideration in above models. Will check original data to be sure they were always null and not as result of saving data to new csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>DESCRIPTION_TRANSLATED</th>\n",
       "      <th>LOAN_USE</th>\n",
       "      <th>TAGS</th>\n",
       "      <th>all_text</th>\n",
       "      <th>joined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>826815</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>882956</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1119019</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>921931</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>937491</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48433</th>\n",
       "      <td>993758</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49010</th>\n",
       "      <td>877307</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49294</th>\n",
       "      <td>920528</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49521</th>\n",
       "      <td>880729</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49899</th>\n",
       "      <td>877280</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LOAN_ID COUNTRY_NAME  STATUS DESCRIPTION_TRANSLATED LOAN_USE TAGS  \\\n",
       "1003    826815        Kenya       1                                        \n",
       "1228    882956        Kenya       1                                        \n",
       "1416   1119019        Kenya       1                                        \n",
       "1773    921931        Kenya       1                                        \n",
       "2156    937491        Kenya       1                                        \n",
       "...        ...          ...     ...                    ...      ...  ...   \n",
       "48433   993758        Kenya       1                                        \n",
       "49010   877307        Kenya       1                                        \n",
       "49294   920528        Kenya       1                                        \n",
       "49521   880729        Kenya       1                                        \n",
       "49899   877280        Kenya       1                                        \n",
       "\n",
       "      all_text joined_text  \n",
       "1003        []         NaN  \n",
       "1228        []         NaN  \n",
       "1416        []         NaN  \n",
       "1773        []         NaN  \n",
       "2156        []         NaN  \n",
       "...        ...         ...  \n",
       "48433       []         NaN  \n",
       "49010       []         NaN  \n",
       "49294       []         NaN  \n",
       "49521       []         NaN  \n",
       "49899       []         NaN  \n",
       "\n",
       "[151 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiva_kenya[kiva_kenya['joined_text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking empty rows for content\n",
    "kiva_kenya['DESCRIPTION_TRANSLATED'][1003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in original dataset to check content\n",
    "old_kiva = pd.read_csv('/Users/precious/dsi/Group-Project/kivamix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>LOAN_NAME</th>\n",
       "      <th>ORIGINAL_LANGUAGE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>DESCRIPTION_TRANSLATED</th>\n",
       "      <th>FUNDED_AMOUNT</th>\n",
       "      <th>LOAN_AMOUNT</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>IMAGE_ID</th>\n",
       "      <th>VIDEO_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_LENDERS_TOTAL</th>\n",
       "      <th>NUM_JOURNAL_ENTRIES</th>\n",
       "      <th>NUM_BULK_ENTRIES</th>\n",
       "      <th>TAGS</th>\n",
       "      <th>BORROWER_NAMES</th>\n",
       "      <th>BORROWER_GENDERS</th>\n",
       "      <th>BORROWER_PICTURED</th>\n",
       "      <th>REPAYMENT_INTERVAL</th>\n",
       "      <th>DISTRIBUTION_MODEL</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408679</th>\n",
       "      <td>877280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>monthly</td>\n",
       "      <td>field_partner</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LOAN_ID LOAN_NAME ORIGINAL_LANGUAGE DESCRIPTION  \\\n",
       "408679   877280       NaN               NaN         NaN   \n",
       "\n",
       "       DESCRIPTION_TRANSLATED  FUNDED_AMOUNT  LOAN_AMOUNT  STATUS  IMAGE_ID  \\\n",
       "408679                    NaN          275.0        275.0       1       NaN   \n",
       "\n",
       "        VIDEO_ID  ... NUM_LENDERS_TOTAL NUM_JOURNAL_ENTRIES NUM_BULK_ENTRIES  \\\n",
       "408679       NaN  ...                 7                   4                3   \n",
       "\n",
       "       TAGS BORROWER_NAMES BORROWER_GENDERS BORROWER_PICTURED  \\\n",
       "408679  NaN            NaN              NaN               NaN   \n",
       "\n",
       "        REPAYMENT_INTERVAL DISTRIBUTION_MODEL  year  \n",
       "408679             monthly      field_partner  2015  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying empty rows were always empty\n",
    "old_kiva[old_kiva['LOAN_ID']==877280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping empty rows\n",
    "kiva_kenya.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51019 entries, 0 to 51169\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   LOAN_ID                 51019 non-null  int64 \n",
      " 1   COUNTRY_NAME            51019 non-null  object\n",
      " 2   STATUS                  51019 non-null  int64 \n",
      " 3   DESCRIPTION_TRANSLATED  51019 non-null  object\n",
      " 4   LOAN_USE                51019 non-null  object\n",
      " 5   TAGS                    51019 non-null  object\n",
      " 6   all_text                51019 non-null  object\n",
      " 7   joined_text             51019 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "kiva_kenya.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Combined Text Data to Reproduce Logistic Regression Model from Above Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables for models\n",
    "X = kiva_kenya['joined_text']\n",
    "y = kiva_kenya['STATUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.784041\n",
       "0    0.215959\n",
       "Name: STATUS, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null model values\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38264, 292840)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking number of features\n",
    "X_train_tdvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing data added max_features to improve score and produced same results as earlier\n",
    "tdvec = TfidfVectorizer(ngram_range=(1,2), max_features=20_000)\n",
    "X_train_tdvec = tdvec.fit_transform(X_train)\n",
    "X_test_tdvec = tdvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating and fitting logestic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_tdvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8421231444699979\n",
      "Test score: 0.8210113680909448\n"
     ]
    }
   ],
   "source": [
    "# scoring logistic regression\n",
    "print(f'Train score: {logreg.score(X_train_tdvec, y_train)}')\n",
    "print(f'Test score: {logreg.score(X_test_tdvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating predictions from test data\n",
    "logreg_preds = logreg.predict(X_test_tdvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc4UlEQVR4nO3deZhcVbnv8e+vuzOPNEkwJkASDGAICBJCghcIwmOCciTyiAZQcrkgMisOCHqvKJ4oj1c9AsokIKgMJygc4EoYjMyGIYQhJBiSQyAJCZlnyNT93j/27lAJPVSZrq7q2r/P8+ynd609vdXQb9baa6+1FRGYmWVNVakDMDMrBSc/M8skJz8zyyQnPzPLJCc/M8ukmlIHkKtjVefoUt2j1GFYAaJjx1KHYAXYtGUNW7Zu1K6cY+wx3WLlqrq89n3x1c0PR8S4XblesZRV8utS3YPRvU8qdRhWgPohA0odghXg2ddu2OVzrFxVx/MP75XXvtX95/bZ5QsWSVklPzMrfwHUU1/qMHaZk5+ZFSQItkZ+zd5y5uRnZgVzzc/MMicI6ipgWKyTn5kVrB4nPzPLmADqnPzMLItc8zOzzAlgq+/5mVnWBOFmr5llUEBd+899Tn5mVphkhEf75+RnZgUSdezS3AhlwcnPzAqSdHg4+ZlZxiTP+Tn5mVkG1bvmZ2ZZ45qfmWVSIOoq4A0YTn5mVjA3e80scwKxJapLHcYuc/Izs4IkDzm72WtmGeQODzPLnAhRF675mVkG1bvmZ2ZZk3R4tP/U0f6/gZm1KXd4mFlm1fk5PzPLGo/wMLPMqndvr5llTTKxgZOfmWVMILZ6eJuZZU0EfsjZzLJIfsjZzLInqIyaX/v/BmbW5uqoymtpiaSLJc2S9JqkOyV1llQr6VFJc9Ofu+Xsf5mkeZLmSBqbU36opJnptqsltVg1dfIzs4IEoj7yW5ojaQBwETAiIoYD1cAE4FJgakQMBaamn5E0LN1+ADAOuFZSQ8/LdcDZwNB0GdfS93DyM7OCJK+urMlryUMN0EVSDdAVWAycCNyWbr8NGJ+unwjcFRGbI2I+MA8YKak/0DMipkVEAH/IOaZJTn5mVqDkpeX5LEAfSdNzlrMbzhIR7wC/ABYAS4C1EfEIsEdELEn3WQL0Sw8ZACzMCWRRWjYgXd+5vFnu8DCzggQFjfBYEREjGtuQ3ss7ERgMrAHulvSVZs7VWDs6milvlpOfmRWslWZyPg6YHxHLASTdAxwBLJXUPyKWpE3aZen+i4A9c44fSNJMXpSu71zeLDd7zawgEaI+qvJaWrAAGCWpa9o7eyzwOnA/MDHdZyJwX7p+PzBBUidJg0k6Np5Pm8brJY1Kz3N6zjFNcs3PzAqSdHjs+vC2iHhO0p+BGcA24CXgRqA7MFnSmSQJ8uR0/1mSJgOz0/3Pj4i69HTnArcCXYAp6dIsJz8zK1DrvcMjIi4HLt+peDNJLbCx/ScBkxopnw4ML+TaTn5mVpCkw8PD28wsgzyllZllTsMIj/bOyc/MCuYXGJlZ5kTA1nonPzPLmKTZ6+RnZhnUSiM8SsrJrxWM/+pCxp60mAjx1txu/Mf/2Z9vT/onAwa9B0D3HtvYsL6GC08+jJqaei68fA5DD1hPfT3ccOVQZk7frYUr2K66+MJpHD5iEWvWduaci/4NgCGDV3Hhuc/TsUMddfXiN9eP5I25fTjm6Pl8cfzs7ccOHrSaC771Wd6cX0tNTR3nnf0CBw1fSoS49U8H88y0vUr1tUrCj7rkQdI44CqSebpuiogri3m9Uti932Y+f+oizhk/ki2bq7nsF69x9PHLuPK7B2zf56zvzGPjhuSJ+HFfTIYcnnfSSHrVbuGK617hmxNGEBXwP1M5e3TqEB74675855v/2F525sSXuP2uA5k+YwCHHfoOZ02cwSX/+zM89sRgHntiMACD9l7N5d9/gjfn1wIw4eTXWLu2M2eddyJS0KP75pJ8n9KqjGZv0b5BOsngb4HjgWHAKelkhBWnuibo2Kmequp6OnWuZ+WyTjlbgyPHLuOJB/cAYK993uPl55Ka3tpVHdm4roahB6wvQdTZ8trsPVi/odOHyrt23QpAt65bWLmq64e2jznyLR5/atD2z2OP+2/u+nMykCBCrFvfuTgBl7n69D0eLS3lrJg1v5HAvIh4E0DSXSTT18xu9qh2ZuWyTtxz657c9ug0tmyqYsa0Wl6aVrt9+/BD17JmZUcWL0j+sN6c051Rx6zgiSn96PuRzXxs2Ab6fmQTb7zWs1RfIbOuv2kEk340la+dMQMp+Nb3xn5on6P+x9v8+KdjAOjWbQsAE097mYOGL2XJuz347Q2HsWZtl7YMu+SS3t72/+rKYtZdm5p4cAeSzm6Y6HBL/aYihlMc3XtuZdQxKzhj3Ci+cuwRdO5SxzEnvLt9+9HHL+XxB/tt//zIvR9hxdJOXHXXi5z9vXm8/kpP6urK+1/ISnXC8W9ww80j+OqZJ3HDzSO4+MJnd9i+374r2Ly5hrcX9Aaguqqevn3eY9br/bjgW5/j9X/24WtnzChB5KXVWtPYl1oxk19eEwxGxI0RMSIiRnSsan9NiINHrebdd7qwbnVH6rZV8czf+vLxT6wFoKq6niOOW86TD3+Q/Orrqvjdz4dy4cmH8ZOLDqRbj2288/aHm1tWfMcd8ybPTEumh3vqmb3Yd+jKHbYfvVOTd936TmzaVM0/nk2OefIfe/OxfVa1WbzlpBKavcVMfk1NPFhRli/pxP4HraVT5zogOPjw1Syc3w2AQ0atZtH8rqxc+kFS79S5jk5dkll4Dhm9ivo6sfDNbqUIPfNWrurCQcOXAnDwQe+yeHGP7duk4MgjFvDEU3vnHCGefWHg9mMOOehdFizs1ZYhl4WG3t72XvMr5j2/F4Ch6aSD75C8denUIl6vJObM7MXTj/bj6snTqdsm3vxnd6bc/VEAjjr+g46OBr1qt/Dv179CfYiVyzrxi8sqsg+o7Fz67ac4aPhSevbczB9vvoc/3XkQV/12FOecNZ3q6nq2bK3mqmsP377/gQcsZcXKrry7tMcO57nltkP47sX/4JyzprNmbWd+dfXotv4qZaESenuVvOyoSCeXPgv8muRRl1vSubia1KtD3xjd+6SixWOtr35Ii++JsTLy7Gs3sG7jO7tUJdtt/37x6Vu+mNe+93zquhebeodHqRX1Ob+IeBB4sJjXMLO2V+5N2nx4hIeZFcQjPMwss5z8zCxzPJmpmWVWuT/Dlw8nPzMrSARs82SmZpZFbvaaWeb4np+ZZVYlzD/p5GdmBXOHh5llToTv+ZlZJok69/aaWRb5np+ZZY7H9ppZNkVy36+9c/Izs4K5t9fMMifc4WFmWeVmr5llknt7zSxzIpz8zCyj/KiLmWVSJdzza/9dNmbWpgJRX1+V19ISSb0l/VnSPyW9Lmm0pFpJj0qam/7cLWf/yyTNkzRH0tic8kMlzUy3XS2pxaqpk5+ZFSzyXPJwFfBQROwPfAJ4HbgUmBoRQ4Gp6WckDQMmAAcA44BrJVWn57kOOBsYmi7jWrqwk5+ZFSbt8MhnaY6knsBRwM0AEbElItYAJwK3pbvdBoxP108E7oqIzRExH5gHjJTUH+gZEdMiIoA/5BzTJCc/Mytc/lW/PpKm5yxn55xlCLAc+L2klyTdJKkbsEdELAFIf/ZL9x8ALMw5flFaNiBd37m8We7wMLOCFfCoy4qIGNHEthrgk8CFEfGcpKtIm7hNaOyi0Ux5s5pMfpKuae4EEXFRSyc3s8oTQH19qzzqsghYFBHPpZ//TJL8lkrqHxFL0ibtspz998w5fiCwOC0f2Eh5s5qr+U3PL34zy5QAWuE5v4h4V9JCSftFxBzgWGB2ukwErkx/3pcecj9wh6RfAR8l6dh4PiLqJK2XNAp4DjgduKal6zeZ/CLittzPkrpFxMaCv6GZVZxWfM7vQuB2SR2BN4EzSPoiJks6E1gAnJxcM2ZJmkySHLcB50dEXXqec4FbgS7AlHRpVov3/CSNJumN6Q7sJekTwNcj4rxCvqGZVZBWSn4R8TLQ2D3BY5vYfxIwqZHy6cDwQq6dT2/vr4GxwMr0Iq+QdE+bWSbl95hLuY//zau3NyIW7vTAdF1T+5pZBlTA8LZ8kt9CSUcAkbbLLyJ5CtvMsiggWqe3t6TyafaeA5xP8tDgO8DB6WczyyzluZSvFmt+EbECOK0NYjGz9qICmr0t1vwkDZH0gKTlkpZJuk/SkLYIzszKVCvObFAq+TR77wAmA/1JHiy8G7izmEGZWRlreMg5n6WM5ZP8FBF/jIht6fInyj6nm1kxReS3lLPmxvbWpquPSboUuIsk6X0Z+GsbxGZm5aoCenub6/B4kR1nTPh6zrYAflKsoMysvKnMa3X5aG5s7+C2DMTM2ol20JmRj7xGeEgaDgwDOjeURcQfihWUmZWz8u/MyEc+ExtcDowhSX4PAscDT5NMFW1mWVQBNb98enu/SDLDwrsRcQbJS0Y6FTUqMytv9XkuZSyfZu/7EVEvaVv6wpFlJHPvm1kWtdJkpqWWT/KbLqk38DuSHuANwPPFDMrMyltF9/Y2yJm09HpJD5G8Iu7V4oZlZmWtkpOfpE82ty0iZhQnJDOz4muu5vfLZrYF8OlWjoXYVkfdylWtfVoroodn/r3UIVgBRo5d2Srnqehmb0Qc05aBmFk7EVT88DYzs8ZVcs3PzKwpFd3sNTNrUgUkv3xmcpakr0j6Yfp5L0kjix+amZWtjMzkfC0wGjgl/bwe+G3RIjKzsqbIfyln+TR7D4+IT0p6CSAiVqevsDSzrMpIb+9WSdWklVhJfSn7IctmVkzlXqvLRz7N3quBe4F+kiaRTGf106JGZWblrQLu+eUztvd2SS+STGslYHxEvF70yMysPLWD+3n5yGcy072A94AHcssiYkExAzOzMpaF5EfypraGFxl1BgYDc4ADihiXmZUxVcBd/3yavQfmfk5ne/l6E7ubmbULBY/wiIgZkg4rRjBm1k5kodkr6Vs5H6uATwLLixaRmZW3rHR4AD1y1reR3AP8S3HCMbN2odKTX/pwc/eI+G4bxWNm7UElJz9JNRGxrbnp7M0se0Tl9/Y+T3J/72VJ9wN3AxsbNkbEPUWOzczKUYbu+dUCK0ne2dHwvF8ATn5mWVUBya+5sb390p7e14CZ6c9Z6c/X2iA2MytXrTi2V1K1pJck/b/0c62kRyXNTX/ulrPvZZLmSZojaWxO+aGSZqbbrpbU4rQzzSW/aqB7uvTIWW9YzCyjWnk+v28AufMFXApMjYihwNT0M5KGARNIRpeNA65NO2UBrgPOBoamy7iWLtpcs3dJRFyRd/hmlh2t1OyVNBD4HDAJaHim+ERgTLp+G/A48L20/K6I2AzMlzQPGCnpLaBnRExLz/kHYDwwpblrN5f82v9shWbW+qKg3t4+kqbnfL4xIm7M+fxr4BJ2fJ54j4hYAhARSyT1S8sHAM/m7LcoLduaru9c3qzmkt+xLR1sZhmVf81vRUSMaGyDpBOAZRHxoqQxeZyrsQpZNFPerOZeWr4qj2DMLINa6VGXTwGfl/RZkhmjekr6E7BUUv+01tcfWJbuvwjYM+f4gcDitHxgI+XNymcmZzOzHbVCb29EXBYRAyNiEElHxt8j4ivA/cDEdLeJwH3p+v3ABEmdJA0m6dh4Pm0ir5c0Ku3lPT3nmCb5vb1mVpjiT1F/JTBZ0pnAAuBkgIiYJWkyMJtknoHzI6IuPeZc4FagC0lHR7OdHeDkZ2YFEq0/wiMiHifp1SUiVtJEn0NETCLpGd65fDowvJBrOvmZWcGyMrzNzGxHTn5mlklOfmaWORma1cXMbEdOfmaWRZU+mamZWaPc7DWz7Cn+Q85twsnPzArn5GdmWVOMER6l4ORnZgVTffvPfk5+ZlYY3/Mzs6xys9fMssnJz8yyyDU/M8smJz8zy5zC3t5Wtpz8zKwgfs7PzLIr2n/2c/Izs4K55mfbVVUF1zz0BiuXdOCHE4fQo/c2vn/92+wxcAtLF3Vk0tf3ZsPaGo75wmpOPm/Z9uMGf3wT54/dlzdndSlh9Nlw7019mHL77kTA8aet4qSvLeePv/gIU+6opVdt8hKwMy5bzMhj17N1i7jqkoHMfbUrqoJzr3iHTxyxAYDvnzqEVcs6ULcNhh++kQt+uojq6lJ+szbmh5ybJ+kWoOGN7AW9Vak9Gn/WChbO7UzX7skf0ZcuWMZLT3dn8m/24EsXLOXLFyzj5kkf5bF7d+Oxe3cDYND+7/Oj37/lxNcG3vpnZ6bcvjtX//UNOnQMvn/qPhx+7FoAvvC15Zx87vId9p9y++4A3PD3OaxZUcMPThvCNVPeoKoKfnDDW3TrUU8E/ORrg3jqgd6MGb+mrb9SSVVCh0cxX1p+KzCuiOcvG336b2HkseuYckft9rLRY9fxt8nJ579NrmX0uHUfOu6Y8Wt4/L96t1WYmbZgbic+/sn36Nw1qK6Bg0Zv4JkpvZve/41OHHJkUtPr3Wcb3XvV8cYrXQHo1iP5y6/bBtu2KOkByBjV57eUs6Ilv4h4ElhVrPOXk3N+vJib/r0/Uf/BX8FufbayalkHAFYt60Dv3bd96LijPr+Gx5z82sSg/Tcx87lurFtVzab3xAt/78nyxcl/nwd+35dzjt2PX168J+vXJO3XIQdsYtrDvajbBu8u6MjcV7tu3x/g+6cM4csHDadL93qOPGFNKb5S6QRJh0c+SxkrZs0vL5LOljRd0vStbC51OAU7/Lh1rFlRw7yZXQs6br9DNrL5/SrenuMmb1vYa+hmvnTeMi6bsA8/OG0fBg97n+qa4ISJK/j9tNlc++gcavfYyo0//igAYyespE//LVwwbj+u++EAho3YSHX1B3/MP73zTe58aRZbt4iXn+5eqq9VMor8lnJW8g6PiLgRuBGgp2rL/Nf1YcMO28ioz6zjsGNn07FT0LVHHZdc8zarV3Sgtl9S+6vtt5U1K3f8VY850U3etjbu1FWMOzVpjNzys/707b+F3fp+UCM//rRV/PD0wQBU1yQ1+gbf/LehDBiy4z/OHTsHoz+zlmkP9+LQoze0wTcoI+3uL/XDSl7za+9+/7P+fGXEMCYePoyfnbs3rzzdnZ9fuDfPPtKT476U/KEd96VVTHu45/ZjpODIE9by+H29SxR1Nq1ZkfwDtGxRB555sBdjxq9h5dIP/lH6x5ReDNpvEwCb3hOb3kv+PF58ojvVNcHe+27m/Y1V24+p2wbPT+3Jnh9rfy2WXdHwkLNrftao//xNP35w/duMm7CKZe8kj7o0OHDURlYs6cC7CzqVMMLsueKsQaxfXUN1h+CCny6iR+86fn7hXvz3rC5IsMfALVz084UArFnZgR+cMgRVwe4f2col17wNwKb3qvjR/xzC1i2irg4O/tQGTjh9RSm/VtuLqIjJTBVFuikp6U5gDNAHWApcHhE3N3dMT9XG4Tq2KPFYcTy8+OVSh2AFGDl2IdNf2bRL/dM9eg+MQ476Rl77PvXAJS9GxIhduV6xFK3mFxGnFOvcZlZa5d6kzYebvWZWmAAqoNnr5GdmhWv/uc/Jz8wK52avmWVSJfT2OvmZWWE8q4uZZVHykHP7z35OfmZWuDKfsSUfTn5mVrBKqPl5bK+ZFSYKWJohaU9Jj0l6XdIsSd9Iy2slPSppbvpzt5xjLpM0T9IcSWNzyg+VNDPddrWkFkexOPmZWYGSsb35LC3YBnw7Ij4OjALOlzQMuBSYGhFDganpZ9JtE4ADSCZKvlZSwwsErgPOBoamS4sTKTv5mVnhWmEy04hYEhEz0vX1wOvAAOBE4LZ0t9uA8en6icBdEbE5IuYD84CRkvoDPSNiWiSTFfwh55gm+Z6fmRWmsJeW95E0PefzjekcnjuQNAg4BHgO2CMilkCSICX1S3cbADybc9iitGxrur5zebOc/MyscPl3eKxoaVYXSd2BvwDfjIh1zdyua2xDNFPeLDd7zaxwrdDhASCpA0niuz0i7kmLl6ZNWdKfDe96XQTsmXP4QGBxWj6wkfJmOfmZWcFUX5/X0uw5kirezcDrEfGrnE33AxPT9YnAfTnlEyR1kjSYpGPj+bSJvF7SqPScp+cc0yQ3e82sMEFrPeT8KeCrwExJL6dl3weuBCZLOhNYAJwMEBGzJE0GZpP0FJ8fEXXpceeSvC63CzAlXZrl5GdmBRHRKg85R8TTNP3W40andI+IScCkRsqnA8MLub6Tn5kVrgJGeDj5mVnhnPzMLHNa755fSTn5mVnBWurJbQ+c/MysQC0PXWsPnPzMrDCBk5+ZZVT7b/U6+ZlZ4SphMlMnPzMrnJOfmWVOBNS1/3avk5+ZFc41PzPLJCc/M8ucAFp+P0fZc/IzswIFhO/5mVnWBO7wMLOM8j0/M8skJz8zyx5PbGBmWRSAp7Qys0xyzc/MssfD28wsiwLCz/mZWSZ5hIeZZZLv+ZlZ5kS4t9fMMso1PzPLniDq6kodxC5z8jOzwnhKKzPLLD/qYmZZE0C45mdmmROezNTMMqoSOjwUZdRlLWk58Hap4yiCPsCKUgdhBanU/2Z7R0TfXTmBpIdIfj/5WBER43blesVSVsmvUkmaHhEjSh2H5c//zSpfVakDMDMrBSc/M8skJ7+2cWOpA7CC+b9ZhfM9PzPLJNf8zCyTnPzMLJOc/IpI0jhJcyTNk3RpqeOxlkm6RdIySa+VOhYrLie/IpFUDfwWOB4YBpwiaVhpo7I83AqU5UO51rqc/IpnJDAvIt6MiC3AXcCJJY7JWhARTwKrSh2HFZ+TX/EMABbmfF6UlplZGXDyKx41UubniszKhJNf8SwC9sz5PBBYXKJYzGwnTn7F8wIwVNJgSR2BCcD9JY7JzFJOfkUSEduAC4CHgdeByRExq7RRWUsk3QlMA/aTtEjSmaWOyYrDw9vMLJNc8zOzTHLyM7NMcvIzs0xy8jOzTHLyM7NMcvJrRyTVSXpZ0muS7pbUdRfOdaukL6brNzU36YKkMZKO+Beu8ZakD73lq6nynfbZUOC1fiTpO4XGaNnl5Ne+vB8RB0fEcGALcE7uxnQmmYJFxFkRMbuZXcYABSc/s3Lm5Nd+PQV8LK2VPSbpDmCmpGpJ/1fSC5JelfR1ACV+I2m2pL8C/RpOJOlxSSPS9XGSZkh6RdJUSYNIkuzFaa3zSEl9Jf0lvcYLkj6VHru7pEckvSTpBhof37wDSf8l6UVJsySdvdO2X6axTJXUNy3bR9JD6TFPSdq/VX6bljk1pQ7ACiephmSewIfSopHA8IiYnyaQtRFxmKROwDOSHgEOAfYDDgT2AGYDt+x03r7A74Cj0nPVRsQqSdcDGyLiF+l+dwD/ERFPS9qLZBTLx4HLgacj4gpJnwN2SGZN+F/pNboAL0j6S0SsBLoBMyLi25J+mJ77ApIXC50TEXMlHQ5cC3z6X/g1WsY5+bUvXSS9nK4/BdxM0hx9PiLmp+WfAQ5quJ8H9AKGAkcBd0ZEHbBY0t8bOf8o4MmGc0VEU/PaHQcMk7ZX7HpK6pFe46T02L9KWp3Hd7pI0hfS9T3TWFcC9cB/puV/Au6R1D39vnfnXLtTHtcw+xAnv/bl/Yg4OLcgTQIbc4uACyPi4Z32+ywtT6mlPPaB5HbJ6Ih4v5FY8h4vKWkMSSIdHRHvSXoc6NzE7pFed83OvwOzf4Xv+VWeh4FzJXUAkLSvpG7Ak8CE9J5gf+CYRo6dBhwtaXB6bG1avh7okbPfIyRNUNL9Dk5XnwROS8uOB3ZrIdZewOo08e1PUvNsUAU01F5PJWlOrwPmSzo5vYYkfaKFa5g1ysmv8txEcj9vRvoSnhtIavj3AnOBmcB1wBM7HxgRy0nu090j6RU+aHY+AHyhocMDuAgYkXaozOaDXucfA0dJmkHS/F7QQqwPATWSXgV+Ajybs20jcICkF0nu6V2Rlp8GnJnGNwu/GsD+RZ7VxcwyyTU/M8skJz8zyyQnPzPLJCc/M8skJz8zyyQnPzPLJCc/M8uk/w+JiNELYtxZhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# showing predictions vs true status where 0=expired and 1=funded\n",
    "logreg_cm = confusion_matrix(y_test, logreg_preds)\n",
    "ConfusionMatrixDisplay(logreg_cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6391781306715063\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, logreg_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.32      0.44      2755\n",
      "           1       0.84      0.96      0.89     10000\n",
      "\n",
      "    accuracy                           0.82     12755\n",
      "   macro avg       0.76      0.64      0.66     12755\n",
      "weighted avg       0.80      0.82      0.79     12755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, logreg_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used NLP (natural language processing) to see if we could determine whether or not a Kiva loan would be funded based off the words used in the loan request. My independent variable was a combination of the loan description translated into English, the tags associated with the loan and a short description of what the loan will be used for. My dependent variable is whether a loan was funded or expired. For my model, I only focused on loans from Kenya. \\\n",
    "\\\n",
    "To preprocess the data, I removed punctuation from the text and plural forms of words to make the words more uniform. I also removed common english words that don't lend much to predictions. I turned my text into a structured numeric dataframe using Tf-IDF Vectorizer which scores words on their importance when used in one document relative to all other documents and gives more weight to words used often in one document but not used in others. I used an ngram range of (1,2) looking at not only single words but also words combined with one of their adjacent words. I also used a max of the top 20,000 features when creating the corpus. \\\n",
    "\\\n",
    "For modeling, I used a logistic regression model to predict the outcome of a loan and classify it as either funded or expired. On the training data we received a score of .84 and on testing data we have a score of .82. This shows that our model has a good fit on the data. It's also an improvement of 4pts above our null model which had a score of .78. This tells us there are word combinations in the data that aren't impacting the dependent variable, but the model is performing at a decent rate where 1.0 is perfect. \\\n",
    "\\\n",
    "We have an accuracy rate of .82 when predicting whether a loan will be funded or expired. Our model performs well when predicting funded models but not so much when predicting a loan will expire without being funded. For our funded predictions, we have a precision score of .84 which tells us out of all the loans we predicted to be funded 84% actually were. Our recall score is .96, which tells us that out of all the loans that were funded we predicted 96% of those correctly. Our f1-score of .89 is a weighted average of the precision and recall scores and tells us how well we did when predicting a loan will be funded. \\\n",
    "\\\n",
    "Unfortunately, the model does not perform as well for expired loans. Only 68% of our predictions for expired loans were correct, and out of all the loans that were expired, we only predicted 32% of those correctly. Our weighted average for expired loans, or f1-score is .44. \\\n",
    "\\\n",
    "Since we are more interested in finding words that improve the chances of a loan being funded, I created a dataframe showing how each word impacts the prediction. From the dataframe, we can see that smaller loans are more likely to be funded, specifically loan with kes 20,000 (with kes meaning Kenyan Shillings) and kes 30,000 in the description are more likely to be funded than any other amount, and requests for kes 60,000 or more negatively impact your chances of funding. Also, women borrowers are more likely to receive full funding than men. Specifically the following word combinations have a positive impact on funding: single parent, single mother/mom, widow, group farmer and the user_favorite tag. Some word combinations that negatively impact funding are mentioning man, married man or father. Mentioning motorcycle or shop also negatively impact the likelihood of funding. Something else to take note of is that parent by itself negatively impacts funding but single parent has one of the 6th highest positive impact score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe of coefs\n",
    "word_coef_df = pd.DataFrame({'coefs': logreg.coef_[0], 'features': tdvec.get_feature_names()})\n",
    "# word_coef_df.sort_values(by='coefs',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefs</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.657314</td>\n",
       "      <td>20 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3.543666</td>\n",
       "      <td>30 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10362</th>\n",
       "      <td>3.511416</td>\n",
       "      <td>kes 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3.492091</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18866</th>\n",
       "      <td>3.455082</td>\n",
       "      <td>user_favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17068</th>\n",
       "      <td>3.174807</td>\n",
       "      <td>single parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17065</th>\n",
       "      <td>2.825557</td>\n",
       "      <td>single mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19509</th>\n",
       "      <td>2.508753</td>\n",
       "      <td>widowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2.384389</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19505</th>\n",
       "      <td>2.325668</td>\n",
       "      <td>widow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2.314914</td>\n",
       "      <td>15 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.199607</td>\n",
       "      <td>10 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10349</th>\n",
       "      <td>2.174054</td>\n",
       "      <td>kes 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10365</th>\n",
       "      <td>2.149991</td>\n",
       "      <td>kes 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17064</th>\n",
       "      <td>2.109488</td>\n",
       "      <td>single mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19192</th>\n",
       "      <td>2.097418</td>\n",
       "      <td>wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18883</th>\n",
       "      <td>2.078606</td>\n",
       "      <td>user_favorite user_favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>2.044095</td>\n",
       "      <td>group farmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>2.039109</td>\n",
       "      <td>grew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19328</th>\n",
       "      <td>1.925584</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coefs                     features\n",
       "95     5.657314                       20 000\n",
       "178    3.543666                       30 000\n",
       "10362  3.511416                       kes 20\n",
       "94     3.492091                           20\n",
       "18866  3.455082                user_favorite\n",
       "17068  3.174807                single parent\n",
       "17065  2.825557                single mother\n",
       "19509  2.508753                      widowed\n",
       "177    2.384389                           30\n",
       "19505  2.325668                        widow\n",
       "74     2.314914                       15 000\n",
       "30     2.199607                       10 000\n",
       "10349  2.174054                      kes 000\n",
       "10365  2.149991                       kes 30\n",
       "17064  2.109488                   single mom\n",
       "19192  2.097418                           wa\n",
       "18883  2.078606  user_favorite user_favorite\n",
       "8055   2.044095                 group farmer\n",
       "8022   2.039109                         grew\n",
       "19328  1.925584                        water"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 features and their coefficients\n",
    "word_coef_df.nlargest(20, 'coefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefs</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-4.469611</td>\n",
       "      <td>100 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-4.370316</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11990</th>\n",
       "      <td>-4.066097</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-2.806446</td>\n",
       "      <td>150 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-2.754315</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-2.738951</td>\n",
       "      <td>80 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-2.732628</td>\n",
       "      <td>120 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16941</th>\n",
       "      <td>-2.706685</td>\n",
       "      <td>shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>-2.685128</td>\n",
       "      <td>nhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10351</th>\n",
       "      <td>-2.659260</td>\n",
       "      <td>kes 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-2.604753</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-2.534121</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13837</th>\n",
       "      <td>-2.485459</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-2.353263</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>-2.320343</td>\n",
       "      <td>father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-2.308350</td>\n",
       "      <td>60 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>-2.267372</td>\n",
       "      <td>married man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12874</th>\n",
       "      <td>-2.260192</td>\n",
       "      <td>motorcycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>-2.257945</td>\n",
       "      <td>kes 120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11347</th>\n",
       "      <td>-2.192215</td>\n",
       "      <td>loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coefs     features\n",
       "42    -4.469611      100 000\n",
       "41    -4.370316          100\n",
       "11990 -4.066097          man\n",
       "81    -2.806446      150 000\n",
       "80    -2.754315          150\n",
       "289   -2.738951       80 000\n",
       "60    -2.732628      120 000\n",
       "16941 -2.706685         shop\n",
       "13195 -2.685128          nhe\n",
       "10351 -2.659260      kes 100\n",
       "288   -2.604753           80\n",
       "59    -2.534121          120\n",
       "13837 -2.485459       parent\n",
       "274   -2.353263           70\n",
       "6690  -2.320343       father\n",
       "248   -2.308350       60 000\n",
       "12219 -2.267372  married man\n",
       "12874 -2.260192   motorcycle\n",
       "10355 -2.257945      kes 120\n",
       "11347 -2.192215         loan"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowest 20 features and their coefficients\n",
    "word_coef_df.nsmallest(20, 'coefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
